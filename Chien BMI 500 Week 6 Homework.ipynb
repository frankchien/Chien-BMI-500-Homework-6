{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a54c9c6-9929-4b56-909b-5ca6da52e25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in a prospective, double-blind, randomized trial the efficacy of a heparinoid in ointment form was assessed in treating superficial thrombophlebitis developing after continuous intravenous infusion. one hundred surgical patients were studied, and clinical examination and the iodine-125-labelled fibrinogen test used to assess the results. the mean time required for the relief of local symptoms and signs and the rate of local decline in radioactivity differed significantly between patients receiving the heparinoid cream and those recieving the placebo.', 'in a randomized, double-blind five-year trial, we tested the efficacy of simultaneously elevating serum levels of high-density lipoprotein (hdl) cholesterol and lowering levels of non-hdl cholesterol with gemfibrozil in reducing the risk of coronary heart disease in 4081 asymptomatic middle-aged men (40 to 55 years of age) with primary dyslipidemia (non-hdl cholesterol greater than or equal to 200 mg per deciliter [5.2 mmol per liter] in two consecutive pretreatment measurements). one group (2051 men) received 600 mg of gemfibrozil twice daily, and the other (2030 men) received placebo. gemfibrozil caused a marked increase in hdl cholesterol and persistent reductions in serum levels of total, low-density lipoprotein (ldl), and non-hdl cholesterol and triglycerides. there were minimal changes in serum lipid levels in the placebo group. the cumulative rate of cardiac end points at five years was 27.3 per 1,000 in the gemfibrozil group and 41.4 per 1,000 in the placebo group--a reduction of 34.0 percent in the incidence of coronary heart disease (95 percent confidence interval, 8.2 to 52.6; p less than 0.02; two-tailed test). the decline in incidence in the gemfibrozil group became evident in the second year and continued throughout the study. there was no difference between the groups in the total death rate, nor did the treatment influence the cancer rates. the results are in accord with two previous trials with different pharmacologic agents and indicate that modification of lipoprotein levels with gemfibrozil reduces the incidence of coronary heart disease in men with dyslipidemia.']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Frank Chien\n",
    "BMI 500 Week 6, NLP\n",
    "Vectorizing and part of speech tagging\n",
    "\"\"\"\n",
    "\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#1a: opening, reading, and lower casing the two provided files\n",
    "abstract1 = open('./text1').read().lower()\n",
    "abstract2 = open('./text2').read().lower()\n",
    "corpus = [abstract1, abstract2]\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ebfa1cd-904c-44fd-a1df-560baebca959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 285)\t4\n",
      "  (0, 511)\t1\n",
      "  (0, 196)\t1\n",
      "  (0, 108)\t1\n",
      "  (0, 517)\t1\n",
      "  (0, 698)\t1\n",
      "  (0, 619)\t8\n",
      "  (0, 203)\t1\n",
      "  (0, 430)\t3\n",
      "  (0, 274)\t2\n",
      "  (0, 456)\t1\n",
      "  (0, 233)\t1\n",
      "  (0, 722)\t1\n",
      "  (0, 91)\t1\n",
      "  (0, 692)\t1\n",
      "  (0, 591)\t1\n",
      "  (0, 669)\t1\n",
      "  (0, 175)\t1\n",
      "  (0, 48)\t1\n",
      "  (0, 146)\t1\n",
      "  (0, 339)\t1\n",
      "  (0, 333)\t1\n",
      "  (0, 459)\t1\n",
      "  (0, 282)\t1\n",
      "  (0, 594)\t1\n",
      "  :\t:\n",
      "  (1, 562)\t1\n",
      "  (1, 87)\t1\n",
      "  (1, 289)\t1\n",
      "  (1, 47)\t1\n",
      "  (1, 747)\t1\n",
      "  (1, 716)\t1\n",
      "  (1, 507)\t1\n",
      "  (1, 705)\t1\n",
      "  (1, 739)\t1\n",
      "  (1, 189)\t1\n",
      "  (1, 492)\t1\n",
      "  (1, 59)\t1\n",
      "  (1, 68)\t1\n",
      "  (1, 329)\t1\n",
      "  (1, 618)\t1\n",
      "  (1, 420)\t1\n",
      "  (1, 446)\t1\n",
      "  (1, 372)\t1\n",
      "  (1, 362)\t1\n",
      "  (1, 743)\t1\n",
      "  (1, 245)\t1\n",
      "  (1, 544)\t1\n",
      "  (1, 195)\t1\n",
      "  (1, 295)\t1\n",
      "  (1, 403)\t1\n"
     ]
    }
   ],
   "source": [
    "#1b\n",
    "vectorizer = CountVectorizer(ngram_range=(1,3))\n",
    "vectorized_corpus = vectorizer.fit_transform(corpus)\n",
    "print(vectorized_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fa7c94c-6fec-4dbc-a146-748064618b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.        0.3974542]\n",
      " [0.3974542 1.       ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#1c Computer cosign similarity\n",
    "co_sim = cosine_similarity(vectorized_corpus)\n",
    "print(co_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7433ffb1-7713-4056-950f-a7e1945bc393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jaccard distance is 0.9251336898395722\n"
     ]
    }
   ],
   "source": [
    "#1d\n",
    "# to do jaccard distance, first the corpus needs to be tokenized\n",
    "tokenized_abs1 = set(abstract1.split())\n",
    "tokenized_abs2 = set(abstract2.split())\n",
    "print(\"jaccard distance is\", nltk.jaccard_distance(tokenized_abs1, tokenized_abs2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c00650-6e51-47c7-9969-185458380138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "030b9817-c64e-4b22-8b48-b5e507e9a0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "Accuracy of default tagger: 11.411 %\n",
      "===================================\n",
      "Using n= 50 unigram tagger\n",
      "Accuracy of unigram tagger of 50 most freq words: 45.588 %\n",
      "Accuracy of unigram tagger of 50 most freq words and defaulting to nouns: 56.998 %\n",
      "Using n= 200 unigram tagger\n",
      "Accuracy of unigram tagger of 200 most freq words: 59.152 %\n",
      "Accuracy of unigram tagger of 200 most freq words and defaulting to nouns: 69.195 %\n",
      "Using n= 1000 unigram tagger\n",
      "Accuracy of unigram tagger of 1000 most freq words: 73.828 %\n",
      "Accuracy of unigram tagger of 1000 most freq words and defaulting to nouns: 79.702 %\n",
      "===================================\n",
      "Using bigrams\n",
      "Performance of bigram tagger: 16.743 %\n",
      "===================================\n",
      "Using combination tagging strategy using bigram tagger, then backoff to unigram, then backoff to noun default\n",
      "Performance of combination tagger: 83.799 % \n"
     ]
    }
   ],
   "source": [
    "#2A\n",
    "#Importing Brown Corpus\n",
    "#to access brown corups, the following was entered into terminal:\n",
    "#python -m nltk.downloader all\n",
    "#corpus was saved onto /Users/frankchien/nltk_data...\n",
    "#some coding examples obtained from https://www.nltk.org/book/ch05.html\n",
    "\n",
    "from nltk.corpus import brown\n",
    "from pickle import dump\n",
    "brown_tagged_sents = brown.tagged_sents(categories='fiction')\n",
    "\n",
    "freq_dist = nltk.FreqDist(brown.words(categories='fiction'))\n",
    "cond_freq_dist = nltk.ConditionalFreqDist(brown.tagged_words(categories='fiction'))\n",
    "\n",
    "#separating the training set and the testing set\n",
    "train_test_cutoff = int(len(brown_tagged_sents)*0.9) #sets cutoff 90% through dataset\n",
    "train_set = brown_tagged_sents[:train_test_cutoff] #gets first 90% of dataset\n",
    "test_set = brown_tagged_sents[train_test_cutoff:] #gets last 10% of dataset\n",
    "\n",
    "def dumb_tagger(): #everything is a awesome, and a noun\n",
    "    default_tagger = nltk.DefaultTagger('NN')\n",
    "    print(\"Accuracy of default tagger: %4.3f %%\" %(100*default_tagger.evaluate(brown_tagged_sents)))\n",
    "    \n",
    "def unigram_tagger(n):\n",
    "    \"\"\"unigram tagging, ie look-up tagger. This is not context specific, so would give the same answer to 'plant' regardless\n",
    "    of whether its used in 'to plant' or 'a plant'\n",
    "    \"\"\"\n",
    "    print(\"Using n= %d unigram tagger\" %n)\n",
    "    most_common_tokens = freq_dist.most_common(n)\n",
    "    likely_tags = dict((word, cond_freq_dist[word].max()) for (word, _) in most_common_tokens)\n",
    "    baseline_tagger = nltk.UnigramTagger(model=likely_tags)\n",
    "    baseline_backoff_tagger = nltk.UnigramTagger(model=likely_tags,backoff=nltk.DefaultTagger('NN'))\n",
    "    baseline_backoff_tagger = nltk.UnigramTagger(model=likely_tags,backoff=nltk.DefaultTagger('NN'))\n",
    "    \"\"\"this code will allow you to see the tagging of each individual word. For example purposes, the 3rd sentences is used\n",
    "    sents = brown.sents(categories='fiction')[3]\n",
    "    print(baseline_tagger.tag(sents)) #allows you to visualize the tags. Those that were not seen in first n words gets \"None\"\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Accuracy of unigram tagger of %d most freq words: %4.3f %%\" %(n, 100*baseline_tagger.evaluate(brown_tagged_sents))) \n",
    "    print(\"Accuracy of unigram tagger of %d most freq words and defaulting to nouns: %4.3f %%\" %(n, 100*baseline_backoff_tagger.evaluate(brown_tagged_sents)))\n",
    "\n",
    "def bigram_tagger(training_set,test_set):\n",
    "    print (\"Using bigrams\")\n",
    "    bigram_tagger = nltk.BigramTagger(training_set)\n",
    "    print(\"Performance of bigram tagger: %4.3f %%\" %(100*bigram_tagger.evaluate(test_set)))\n",
    "\n",
    "def ultimate_tagger(training_set, test_set, n):\n",
    "    #this function will also pickle the combination tagger\n",
    "    print (\"Using combination tagging strategy using bigram tagger, then backoff to unigram, then backoff to noun default\")\n",
    "    most_common_tokens = freq_dist.most_common(n)\n",
    "    likely_tags = dict((word, cond_freq_dist[word].max()) for (word, _) in most_common_tokens)\n",
    "    baseline_tagger = nltk.UnigramTagger(model=likely_tags)\n",
    "    \n",
    "    unigram_backoff_tagger = nltk.UnigramTagger(model=likely_tags,backoff=nltk.DefaultTagger('NN'))\n",
    "    ultimate_tagger = nltk.BigramTagger(training_set, backoff=unigram_backoff_tagger)\n",
    "    print(\"Performance of combination tagger: %4.3f %% \" %(100*ultimate_tagger.evaluate(test_set)))\n",
    "    \n",
    "    #pickling ultimate_tagger\n",
    "    output = open('ultimate_tagger.pkl', 'wb')\n",
    "    dump(ultimate_tagger, output, -1)\n",
    "    output.close()\n",
    "\n",
    "\n",
    "print(\"===================================\")\n",
    "dumb_tagger()\n",
    "print(\"===================================\")\n",
    "unigram_tagger(50)\n",
    "unigram_tagger(200)\n",
    "unigram_tagger(1000)\n",
    "print(\"===================================\")\n",
    "bigram_tagger(train_set,test_set)\n",
    "print(\"===================================\")\n",
    "ultimate_tagger(train_set,test_set,1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "549462a6-b0d1-405a-9ecb-48b77679eacf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('In', 'IN'),\n",
       " ('a', 'AT'),\n",
       " ('prospective,', 'NN'),\n",
       " ('double-blind,', 'NN'),\n",
       " ('randomized', 'NN'),\n",
       " ('trial', 'NN'),\n",
       " ('the', 'AT'),\n",
       " ('efficacy', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('a', 'AT'),\n",
       " ('heparinoid', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('ointment', 'NN'),\n",
       " ('form', 'NN'),\n",
       " ('was', 'BEDZ'),\n",
       " ('assessed', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('treating', 'NN'),\n",
       " ('superficial', 'NN'),\n",
       " ('thrombophlebitis', 'NN'),\n",
       " ('developing', 'NN'),\n",
       " ('after', 'IN'),\n",
       " ('continuous', 'NN'),\n",
       " ('intravenous', 'NN'),\n",
       " ('infusion.', 'NN'),\n",
       " ('One', 'CD'),\n",
       " ('hundred', 'CD'),\n",
       " ('surgical', 'NN'),\n",
       " ('patients', 'NN'),\n",
       " ('were', 'BED'),\n",
       " ('studied,', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('clinical', 'NN'),\n",
       " ('examination', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('the', 'AT'),\n",
       " ('iodine-125-labelled', 'NN'),\n",
       " ('fibrinogen', 'NN'),\n",
       " ('test', 'NN'),\n",
       " ('used', 'VBN'),\n",
       " ('to', 'TO'),\n",
       " ('assess', 'NN'),\n",
       " ('the', 'AT'),\n",
       " ('results.', 'NN'),\n",
       " ('The', 'AT'),\n",
       " ('mean', 'JJ'),\n",
       " ('time', 'NN'),\n",
       " ('required', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('the', 'AT'),\n",
       " ('relief', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('local', 'JJ'),\n",
       " ('symptoms', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('signs', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('the', 'AT'),\n",
       " ('rate', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('local', 'JJ'),\n",
       " ('decline', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('radioactivity', 'NN'),\n",
       " ('differed', 'NN'),\n",
       " ('significantly', 'NN'),\n",
       " ('between', 'IN'),\n",
       " ('patients', 'NN'),\n",
       " ('receiving', 'NN'),\n",
       " ('the', 'AT'),\n",
       " ('heparinoid', 'NN'),\n",
       " ('cream', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('those', 'DTS'),\n",
       " ('recieving', 'NN'),\n",
       " ('the', 'AT'),\n",
       " ('placebo.', 'NN')]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "using NLTK tagger on text1 from homework assignment\n",
    "\"\"\"\n",
    "from pickle import load\n",
    "\n",
    "input = open('ultimate_tagger.pkl', 'rb')\n",
    "ultimate_tagger = load(input)\n",
    "input.close()\n",
    "\n",
    "abstract1 = open('./text1').read().split()\n",
    "ultimate_tagger.tag(abstract1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950c959a-3633-419c-8395-824d3c1a7e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "can lowercasing affect the performance?\n",
    "\n",
    "Yes, because in natural language, casing can give information with regards to parts of speech. This happens with proper nouns.\n",
    "Examples include: Seattle Seahawks, Philadelphia Eagles, or Venmo. \n",
    "In cases such as Venmo or Google, capitalization can change the part of speech. \n",
    "Consider \"may I venmo you the money?\". Here venmo is used as a verb\n",
    "Alternatively, consider \"I've read that Venmo is owned by PayPal\". Here, Venmo refers to the app and is a noun.\n",
    "\n",
    "Thus, lowercasing all tokens in the a text can effect POS tagger performance because you may lose information on whether or \n",
    "not a token was used referring to the action it represents or as a proper noun.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b1a08d-b329-47c9-81bb-cdd873cbc920",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For question 3 - see the attached write up pdf\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
